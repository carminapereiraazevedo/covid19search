{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all json files, extract abstracts and body, lemmatize terms\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/liamca/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/liamca/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/liamca/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import globals\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize, tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_characters = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 '-\")\n",
    "def remover(my_string = \"\"):\n",
    "    for item in my_string:\n",
    "        if item not in valid_characters:\n",
    "            my_string = my_string.replace(item, \" \")\n",
    "    return my_string\n",
    "\n",
    "def processFile(blob_name):\n",
    "    counter = 0\n",
    "\n",
    "    existing_file = blob_name\n",
    "    if existing_file in processed_files:\n",
    "        print ('Skipping already processed file: ', existing_file)\n",
    "    else:\n",
    "        json_content = {}\n",
    "        content = block_blob_service.get_blob_to_text(globals.blob_container_name, blob_name).content\n",
    "        try:\n",
    "            if not os.path.exists(os.path.dirname(os.path.join(globals.processed_text_dir, blob_name))):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(os.path.join(globals.processed_text_dir, blob_name)))\n",
    "                except OSError as exc: # Guard against race condition\n",
    "                    if exc.errno != errno.EEXIST:\n",
    "                        raise\n",
    "                        \n",
    "            with open(os.path.join(globals.processed_text_dir, blob_name).replace('.json','.txt'), 'w', encoding='utf-8') as f:\n",
    "                json_content = json.loads(content)\n",
    "                if \"abstract\" in json_content:\n",
    "                    for c in json_content[\"abstract\"]:\n",
    "                        f.write(convertTextToTerms(c[\"text\"].strip()) + '\\r\\n')\n",
    "\n",
    "                if \"body_text\" in json_content:\n",
    "                    for c in json_content[\"body_text\"]:\n",
    "                        f.write(convertTextToTerms(c[\"text\"].strip()) + '\\r\\n')\n",
    "\n",
    "        except Exception as ex:\n",
    "            print (blob_name, \" - Error:\", str(ex))\n",
    "        \n",
    "\n",
    "def convertTextToTerms(content):\n",
    "    tagged_sentence = ''\n",
    "    text = word_tokenize(content)\n",
    "    tagged_content = nltk.pos_tag(text)\n",
    "    for (word, tag) in tagged_content:\n",
    "        #print (word.lower(), tag)\n",
    "        if 'JJ' in tag or 'NN' in tag or 'VB' in tag:\n",
    "            parsed_word = word.lower()\n",
    "            if (len(parsed_word) >= globals.minPhraseLen):\n",
    "                parsed_word = lemmatizer.lemmatize(parsed_word)\n",
    "            tagged_sentence += parsed_word + ' '\n",
    "        else: \n",
    "            #tagged_sentence += ' ' + word.lower()\n",
    "            tagged_sentence += ' '\n",
    "    # remove double spaces\n",
    "    while '  ' in tagged_sentence:\n",
    "        tagged_sentence = tagged_sentence.replace('  ', ' ')\n",
    "    return tagged_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files to be procesed\n",
    "block_blob_service = BlockBlobService(account_name=globals.blob_account_name, account_key=globals.blob_account_key)\n",
    "generator = block_blob_service.list_blobs(globals.blob_container_name, globals.blob_container_path)\n",
    "\n",
    "# Add Blobs to a list\n",
    "blobs = []\n",
    "for blob in generator:\n",
    "    blobs.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Resetting dir: \", os.path.join(globals.processed_text_dir, globals.blob_container_path))\n",
    "# globals.resetDir(os.path.join(globals.processed_text_dir, globals.blob_container_path))\n",
    "processed_files = globals.getFilesInDir(globals.processed_text_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertTextToTerms(\"The quick brown foxes jumped over the lazy dogs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Downloading text files and processing (lemmas format)...\")\n",
    "processed_list = Parallel(n_jobs=globals.max_parallelism)(delayed(processFile)(blob_name)\n",
    "    for blob_name in blobs)\n",
    "\n",
    "# blob_counter = 0\n",
    "# for blob_name in blobs:\n",
    "#     processFile(blob_name)\n",
    "#     blob_counter += 1\n",
    "#     if blob_counter % 100 == 0:\n",
    "#         print (\"Processed file count:\", blob_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
