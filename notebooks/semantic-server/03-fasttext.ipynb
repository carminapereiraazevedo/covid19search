{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import globals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import tqdm\n",
    "\n",
    "from nltk import tokenize\n",
    "import json\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from wasabi import msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttextTrainVectors(\n",
    "    fasttext_bin,\n",
    "    in_dir,\n",
    "    out_dir,\n",
    "    n_threads=10,\n",
    "    min_count=50,\n",
    "    vector_size=300,\n",
    "    verbose=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 4: Train the vectors\n",
    "\n",
    "    Expects a directory of preprocessed .s2v input files, will concatenate them\n",
    "    (using a temporary file on disk) and will use fastText to train a word2vec\n",
    "    model. See here for installation instructions:\n",
    "    https://github.com/facebookresearch/fastText\n",
    "\n",
    "    Note that this script will call into fastText and expects you to pass in the\n",
    "    built fasttext binary. The command will also be printed if you want to run\n",
    "    it separately.\n",
    "    \"\"\"\n",
    "    input_path = Path(in_dir)\n",
    "    output_path = Path(out_dir)\n",
    "    if not Path(fasttext_bin).exists():\n",
    "        msg.fail(\"Can't find fastText binary\", fasttext_bin, exits=1)\n",
    "    if not input_path.exists() or not input_path.is_dir():\n",
    "        msg.fail(\"Not a valid input directory\", in_dir, exits=1)\n",
    "    if output_path.exists():\n",
    "        rmtree(output_path)\n",
    "    output_path.mkdir(parents=True)\n",
    "    print(f\"ReCreated vector output directory {out_dir}\")\n",
    "    output_file = output_path / f\"vectors_w2v_{vector_size}dim\"\n",
    "\n",
    "    # fastText expects only one input file and only reads from disk and not\n",
    "    # stdin, so we need to create a temporary file that concatenates the inputs\n",
    "    tmp_path = input_path / \"s2v_input.tmp\"\n",
    "    input_files = [p for p in input_path.iterdir() if p.suffix == \".txt\"]\n",
    "    if not input_files:\n",
    "        print(\"Input directory contains no .s2v files: \" + in_dir)\n",
    "        sys.exit()\n",
    "    with tmp_path.open(\"a\", encoding=\"utf8\") as tmp_file:\n",
    "        for input_file in input_files:\n",
    "            with input_file.open(\"r\", encoding=\"utf8\") as f:\n",
    "                tmp_file.write(f.read())\n",
    "                tmp_file.write(f.read())\n",
    "    msg.info(\"Created temporary merged input file\", tmp_path)\n",
    "\n",
    "    #tmp_path = input_path / \"content.merged\"\n",
    "    \n",
    "    msg.info(\"Training vectors\")\n",
    "    cmd = (\n",
    "        f\"{fasttext_bin} skipgram -thread {n_threads} -input {tmp_path} \"\n",
    "        f\"-output {output_file} -dim {vector_size} -minn 0 -maxn 0 \"\n",
    "        f\"-minCount {min_count} -verbose {verbose}\"\n",
    "    )\n",
    "    print(cmd)\n",
    "    train_cmd = os.system(cmd)\n",
    "    tmp_path.unlink()\n",
    "    msg.good(\"Deleted temporary input file\", tmp_path)\n",
    "    if train_cmd != 0:\n",
    "        msg.fail(\"Failed training vectors\", exits=1)\n",
    "    msg.good(\"Successfully trained vectors\", out_dir)\n",
    "\n",
    "    msg.info(\"Creating vocabulary\")\n",
    "    vocab_file = output_path / \"vocab.txt\"\n",
    "    cmd = f\"{fasttext_bin} dump {output_file.with_suffix('.bin')} dict > {vocab_file}\"\n",
    "    print(cmd)\n",
    "    vocab_cmd = os.system(cmd)\n",
    "    if vocab_cmd != 0:\n",
    "        msg.fail(\"Failed creating vocabulary\", exits=1)\n",
    "    msg.good(\"Successfully created vocabulary file\", vocab_file)\n",
    "\n",
    "def _get_shape(file_):\n",
    "    \"\"\"Return a tuple with (number of entries, vector dimensions). Handle\n",
    "    both word2vec/FastText format, which has a header with this, or GloVe's\n",
    "    format, which doesn't.\"\"\"\n",
    "    first_line = next(file_).split()\n",
    "    if len(first_line) == 2:\n",
    "        return tuple(int(size) for size in first_line), file_\n",
    "    count = 1\n",
    "    for line in file_:\n",
    "        count += 1\n",
    "    file_.seek(0)\n",
    "    shape = (count, len(first_line) - 1)\n",
    "    return shape, file_\n",
    "\n",
    "\n",
    "def export(in_file, vocab_file, out_dir):\n",
    "    \"\"\"\n",
    "    Step 5: Export a sense2vec component\n",
    "\n",
    "    Expects a vectors.txt and a vocab file trained with GloVe and exports\n",
    "    a component that can be loaded with Sense2vec.from_disk.\n",
    "    \"\"\"\n",
    "    input_path = Path(in_file)\n",
    "    vocab_path = Path(vocab_file)\n",
    "    output_path = Path(out_dir)\n",
    "    if not input_path.exists():\n",
    "        print(\"Can't find input file: \" + in_file)\n",
    "        sys.exit()\n",
    "    if input_path.suffix == \".bin\":\n",
    "        print(\"Need text-based vectors file, not binary: \" + in_file)\n",
    "        sys.exit()\n",
    "    if not vocab_path.exists():\n",
    "        print(\"Can't find vocab file: \" + vocab_file)\n",
    "        sys.exit()\n",
    "    if output_path.exists():\n",
    "        rmtree(output_path)\n",
    "    output_path.mkdir(parents=True)\n",
    "    print(f\"ReCreated vector output directory {out_dir}\")\n",
    "    with input_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "        (n_vectors, vector_size), f = _get_shape(f)\n",
    "        vectors_data = f.readlines()\n",
    "    with vocab_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "        vocab_data = f.readlines()\n",
    "    data = []\n",
    "    all_senses = set()\n",
    "    for item in vectors_data:\n",
    "        item = item.rstrip().rsplit(\" \", vector_size)\n",
    "        key = item[0]\n",
    "        try:\n",
    "            _, sense = split_key(key)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        vec = item[1:]\n",
    "        if len(vec) != vector_size:\n",
    "            print(f\"Wrong vector size: {len(vec)} (expected {vector_size})\")\n",
    "            sys.exit()\n",
    "        all_senses.add(sense)\n",
    "        data.append((key, numpy.asarray(vec, dtype=numpy.float32)))\n",
    "    s2v = Sense2Vec(shape=(len(data), vector_size), senses=all_senses)\n",
    "    for key, vector in data:\n",
    "        s2v.add(key, vector)\n",
    "    for item in vocab_data:\n",
    "        item = item.rstrip()\n",
    "        if item.endswith(\" word\"):  # for fastText vocabs\n",
    "            item = item[:-5]\n",
    "        try:\n",
    "            key, freq = item.rsplit(\" \", 1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        s2v.set_freq(key, int(freq))\n",
    "    print(\"Created the sense2vec model\")\n",
    "    print(f\"{len(data)} vectors, {len(all_senses)} total senses\")\n",
    "    s2v.to_disk(output_path)\n",
    "    print(\"Saved model to directory\", out_dir)\n",
    "\n",
    "def precompileCache(\n",
    "    vectors, gpu_id=-1, n_neighbors=100, batch_size=1024, cutoff=0, start=0, end=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 6: Precompute nearest-neighbor queries (optional)\n",
    "\n",
    "    Precompute nearest-neighbor queries for every entry in the vocab to make\n",
    "    Sense2Vec.most_similar faster. The --cutoff option lets you define the\n",
    "    number of earliest rows to limit the neighbors to. For instance, if cutoff\n",
    "    is 100000, no word will have a nearest neighbor outside of the top 100k\n",
    "    vectors.\n",
    "    \"\"\"\n",
    "    if gpu_id == -1:\n",
    "        xp = numpy\n",
    "    else:\n",
    "        import cupy as xp\n",
    "        import cupy.cuda.device\n",
    "\n",
    "        cupy.take_along_axis = take_along_axis\n",
    "        cupy.put_along_axis = put_along_axis\n",
    "        device = cupy.cuda.device.Device(gpu_id)\n",
    "        device.use()\n",
    "    vectors_dir = Path(vectors)\n",
    "    vectors_file = vectors_dir / \"vectors\"\n",
    "    if not vectors_dir.is_dir() or not vectors_file.exists():\n",
    "        err = \"Are you passing in the exported sense2vec directory containing a vectors file?\"\n",
    "        print(f\"Can't load vectors from {vectors}: \" +  err)\n",
    "        sys.exit()\n",
    "    with msg.loading(f\"Loading vectors from {vectors}\"):\n",
    "        vectors = xp.load(str(vectors_file))\n",
    "    msg.good(f\"Loaded {vectors.shape[0]:,} vectors with dimension {vectors.shape[1]}\")\n",
    "    norms = xp.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    # Normalize to unit norm\n",
    "    vectors /= norms\n",
    "    if cutoff < 1:\n",
    "        cutoff = vectors.shape[0]\n",
    "    if end is None:\n",
    "        end = vectors.shape[0]\n",
    "    mean = float(norms.mean())\n",
    "    var = float(norms.var())\n",
    "    msg.good(f\"Normalized (mean {mean:,.2f}, variance {var:,.2f})\")\n",
    "    msg.info(f\"Finding {n_neighbors:,} neighbors among {cutoff:,} most frequent\")\n",
    "    n = min(n_neighbors, vectors.shape[0])\n",
    "    subset = vectors[:cutoff]\n",
    "    best_rows = xp.zeros((end - start, n), dtype=\"i\")\n",
    "    scores = xp.zeros((end - start, n), dtype=\"f\")\n",
    "    for i in tqdm.tqdm(list(range(start, end, batch_size))):\n",
    "        size = min(batch_size, end - i)\n",
    "        batch = vectors[i : i + size]\n",
    "        sims = xp.dot(batch, subset.T)\n",
    "        # Set self-similarities to -inf, so that we don't return them.\n",
    "        indices = xp.arange(i, min(i + size, sims.shape[1])).reshape((-1, 1))\n",
    "        xp.put_along_axis(sims, indices, -xp.inf, axis=1)\n",
    "        # This used to use argpartition, to do a partial sort...But this ended\n",
    "        # up being a ratsnest of terrible numpy crap. Just sorting the whole\n",
    "        # list isn't really slower, and it's much simpler to read.\n",
    "        ranks = xp.argsort(sims, axis=1)\n",
    "        batch_rows = ranks[:, -n:]\n",
    "        # Reverse\n",
    "        batch_rows = batch_rows[:, ::-1]\n",
    "        batch_scores = xp.take_along_axis(sims, batch_rows, axis=1)\n",
    "        best_rows[i : i + size] = batch_rows\n",
    "        scores[i : i + size] = batch_scores\n",
    "    msg.info(\"Saving output\")\n",
    "    if not isinstance(best_rows, numpy.ndarray):\n",
    "        best_rows = best_rows.get()\n",
    "    if not isinstance(scores, numpy.ndarray):\n",
    "        scores = scores.get()\n",
    "    output = {\n",
    "        \"indices\": best_rows,\n",
    "        \"scores\": scores.astype(\"float16\"),\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"cutoff\": cutoff,\n",
    "    }\n",
    "    output_file = vectors_dir / \"cache\"\n",
    "    with msg.loading(\"Saving output...\"):\n",
    "        srsly.write_msgpack(output_file, output)\n",
    "    msg.good(f\"Saved cache to {output_file}\")\n",
    "\n",
    "\n",
    "# These functions are missing from cupy, but will be supported in cupy 7.\n",
    "def take_along_axis(a, indices, axis):\n",
    "    \"\"\"Take values from the input array by matching 1d index and data slices.\n",
    "\n",
    "    Args:\n",
    "        a (cupy.ndarray): Array to extract elements.\n",
    "        indices (cupy.ndarray): Indices to take along each 1d slice of ``a``.\n",
    "        axis (int): The axis to take 1d slices along.\n",
    "\n",
    "    Returns:\n",
    "        cupy.ndarray: The indexed result.\n",
    "\n",
    "    .. seealso:: :func:`numpy.take_along_axis`\n",
    "    \"\"\"\n",
    "    import cupy\n",
    "\n",
    "    if indices.dtype.kind not in (\"i\", \"u\"):\n",
    "        raise IndexError(\"`indices` must be an integer array\")\n",
    "\n",
    "    if axis is None:\n",
    "        a = a.ravel()\n",
    "        axis = 0\n",
    "\n",
    "    ndim = a.ndim\n",
    "\n",
    "    if not (-ndim <= axis < ndim):\n",
    "        raise IndexError(\"Axis overrun\")\n",
    "\n",
    "    axis %= a.ndim\n",
    "\n",
    "    if ndim != indices.ndim:\n",
    "        raise ValueError(\"`indices` and `a` must have the same number of dimensions\")\n",
    "\n",
    "    fancy_index = []\n",
    "    for i, n in enumerate(a.shape):\n",
    "        if i == axis:\n",
    "            fancy_index.append(indices)\n",
    "        else:\n",
    "            ind_shape = (1,) * i + (-1,) + (1,) * (ndim - i - 1)\n",
    "            fancy_index.append(cupy.arange(n).reshape(ind_shape))\n",
    "\n",
    "    return a[fancy_index]\n",
    "\n",
    "\n",
    "def put_along_axis(a, indices, value, axis):\n",
    "    import cupy\n",
    "\n",
    "    if indices.dtype.kind not in (\"i\", \"u\"):\n",
    "        raise IndexError(\"`indices` must be an integer array\")\n",
    "\n",
    "    if axis is None:\n",
    "        a = a.ravel()\n",
    "        axis = 0\n",
    "\n",
    "    ndim = a.ndim\n",
    "\n",
    "    if not (-ndim <= axis < ndim):\n",
    "        raise IndexError(\"Axis overrun\")\n",
    "\n",
    "    axis %= a.ndim\n",
    "\n",
    "    if ndim != indices.ndim:\n",
    "        raise ValueError(\"`indices` and `a` must have the same number of dimensions\")\n",
    "\n",
    "    fancy_index = []\n",
    "    for i, n in enumerate(a.shape):\n",
    "        if i == axis:\n",
    "            fancy_index.append(indices)\n",
    "        else:\n",
    "            ind_shape = (1,) * i + (-1,) + (1,) * (ndim - i - 1)\n",
    "            fancy_index.append(cupy.arange(n).reshape(ind_shape))\n",
    "\n",
    "    a[fancy_index] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReCreated vector output directory /data/processed/gaming/fasttext_vectors\n",
      "\u001b[38;5;4mℹ Created temporary merged input file\u001b[0m\n",
      "/data/processed/gaming/merged-terms/s2v_input.tmp\n",
      "\u001b[38;5;4mℹ Training vectors\u001b[0m\n",
      "/data/home/liamca/notebooks/fasttext/fastText/fasttext skipgram -thread 16 -input /data/processed/gaming/merged-terms/s2v_input.tmp -output /data/processed/gaming/fasttext_vectors/vectors_w2v_300dim -dim 300 -minn 0 -maxn 0 -minCount 10 -verbose 2\n",
      "\u001b[38;5;2m✔ Deleted temporary input file\u001b[0m\n",
      "/data/processed/gaming/merged-terms/s2v_input.tmp\n",
      "\u001b[38;5;2m✔ Successfully trained vectors\u001b[0m\n",
      "/data/processed/gaming/fasttext_vectors\n",
      "\u001b[38;5;4mℹ Creating vocabulary\u001b[0m\n",
      "/data/home/liamca/notebooks/fasttext/fastText/fasttext dump /data/processed/gaming/fasttext_vectors/vectors_w2v_300dim.bin dict > /data/processed/gaming/fasttext_vectors/vocab.txt\n",
      "\u001b[38;5;2m✔ Successfully created vocabulary file\u001b[0m\n",
      "/data/processed/gaming/fasttext_vectors/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# Train Vectors (FastText)\n",
    "fasttextTrainVectors(globals.fasttext_bin, globals.merged_text_dir, globals.vectors_out_dir, globals.n_threads, globals.min_count, globals.vector_size, globals.verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
