{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Azure Cognitive Search Service\n",
    "This notebook will set up your Azure Cognitive Search Service for the COVID-19 example described at https://aka.ms/Covid19CognitiveSearchCode.  Data is pulled from two folders in the same Azure blob storage container.  The main indexer runs data in json format through a skillset which reshapes the data and extracts medical entities, and puts the enriched data in the search index.  A second metadata indexer pulls additional metadata into the same search index.   \n",
    "\n",
    "First, you will need an Azure account.  If you don't already have one, you can start a free trial of Azure [here](https://azure.microsoft.com/free/).  \n",
    "\n",
    "Secondly, create a new Azure search service using the Azure portal at <https://portal.azure.com/#create/Microsoft.Search>.  Select your Azure subscription.  You may create a new resource group (you can name it something like \"covid19-search-rg\").  You will need a globally-unique URL as the name of your search service (try something like \"covid19-search-\" plus your name, organization, or numbers).  Finally, choose a nearby location to host your search service - please remember the location that you chose, as your Cognitive Services instance will need to be based in the same location.  Click \"Review + create\" and then (after validation) click \"Create\" to instantiate and deploy the service.  \n",
    "\n",
    "After deployment is complete, click \"Go to resource\" to navigate to your new search service.  We will need some information about your search service to fill in the \"Azure Search variables\" section in the cell below.  First, on the \"Overview\" main page, you should see a \"Url\" value.  Copy that value into the \"azsearch_url\" variable in the cell below (you can just update the \"<YourSearchServiceName>\" section of the URL with the name of your Azure search service).  Then, on the Azure portal page in the left-hand pane under \"Settings\", click on \"Keys\".  Update the azsearch_key value below with one of the keys from your service on the Azure portal page.  \n",
    "\n",
    "Finally, you will need to create an Azure storage account and upload the COVID-19 data set. The data set can be downloaded from https://www.semanticscholar.org/cord19/download. There are two different sections to download: the metadata and document parses. Then, back on the Azure portal, you can create a new Azure storage account at https://portal.azure.com/#create/Microsoft.StorageAccount. Use the same subscription, resource group, and location that you did for the Azure search service. Choose your own unique storage account name (it must be lowercase letters and numbers only). You can change the replication to LRS. You can use the defaults for everything else, and then create the storage. Once it has been deployed, update the blob_connection_string variable in the cell below. Then create a container in your blob storage called \"covid19\". Inside of that container, create a folder called \"json\" and upload the document parses data there. Then create a folder called \"metadata\" in the same blob container, and upload the metadata.csv file to that folder. If you modify those names, update their respective values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Search variables\n",
    "azsearch_url = \"<YourSearchServiceName>.search.windows.net\"  # If you copy this value from the portal, leave off the \"https://\" from the beginning\n",
    "azsearch_key = \"TODO\" \n",
    "\n",
    "# Data source which contains documents to process\n",
    "blob_connection_string = \"DefaultEndpointsProtocol=https;AccountName=TODO;AccountKey=TODO;EndpointSuffix=core.windows.net\"\n",
    "blob_container = \"covid19\"\n",
    "data_folder = \"json\"\n",
    "metadata_folder = \"metadata\"\n",
    "\n",
    "# Prefix for elements of the Cognitive Search service\n",
    "search_prefix = \"covid19\"  # Note that if you change this value, you will also have to change the values in the indexer json.\n",
    "\n",
    "print(\"The variables are initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a simple function to wrap REST requests to the Azure Search service.  If called with no parameters, it will get the service statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def azsearch_rest(request_type=\"GET\", endpoint=\"servicestats\", body=None):\n",
    "    # Imports and constants\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "    # Request headers.\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': azsearch_key\n",
    "    }\n",
    "\n",
    "    # Request parameters\n",
    "    params = urllib.parse.urlencode({\n",
    "        'api-version':'2019-05-06-Preview'\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Execute the REST API call and get the response.\n",
    "        conn = http.client.HTTPSConnection(azsearch_url)\n",
    "        request_path = \"/{0}?{1}\".format(endpoint, params)\n",
    "        conn.request(request_type, request_path, body, headers)\n",
    "        response = conn.getresponse()\n",
    "        print(response.status)\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "        result = None\n",
    "        if len(data) > 0:\n",
    "            result = json.loads(data)\n",
    "        return result\n",
    "\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "        \n",
    "# Test the function\n",
    "try:\n",
    "    response = azsearch_rest()\n",
    "    if response != None:\n",
    "        print(json.dumps(response, sort_keys=True, indent=2))\n",
    "except Exception as ex:\n",
    "    print(ex.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up data sources for your search service.  In this service, we have two data sources, one that pulls data from a json folder and one that pulls data from a metadata folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasource(datasource_name, blob_connection_string, blob_container, folder):\n",
    "\n",
    "    # Define the request body with details of the data source we want to create\n",
    "    body = {   \n",
    "        \"name\": datasource_name,  \n",
    "        \"description\": \"\",  \n",
    "        \"type\": \"azureblob\",\n",
    "        \"credentials\": \n",
    "        { \n",
    "            \"connectionString\": blob_connection_string\n",
    "        },  \n",
    "        \"container\": { \n",
    "            \"name\": blob_container, \n",
    "            \"query\": folder \n",
    "        }\n",
    "    } \n",
    "\n",
    "    try:\n",
    "        # Call the REST API's 'datasources' endpoint to create a data source\n",
    "        result = azsearch_rest(request_type=\"POST\", endpoint=\"datasources\", body=json.dumps(body))\n",
    "        if result != None:\n",
    "            print(json.dumps(result, sort_keys=True, indent=2))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "\n",
    "# Create two datasources\n",
    "datasource_name = search_prefix + \"-ds\"\n",
    "metadata_datasource_name = \"metadata-ds\"\n",
    "\n",
    "create_datasource(datasource_name, blob_connection_string, blob_container, data_folder)\n",
    "create_datasource(metadata_datasource_name, blob_connection_string, blob_container, metadata_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's set up your search index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = search_prefix + \"-index\"\n",
    "\n",
    "# Define the request body\n",
    "with open(\"index.json\") as datafile:\n",
    "  index_json = json.load(datafile)\n",
    "\n",
    "try:\n",
    "    result = azsearch_rest(request_type=\"PUT\", endpoint=\"indexes/\" + index_name, body=json.dumps(index_json))\n",
    "    if result != None:\n",
    "        print(json.dumps(result, sort_keys=True, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up your skillset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillset_name = search_prefix + \"-skillset\"\n",
    "\n",
    "# Define the request body\n",
    "with open(\"skillset.json\") as datafile:\n",
    "  skillset_json = json.load(datafile)\n",
    "\n",
    "try:\n",
    "    result = azsearch_rest(request_type=\"PUT\", endpoint=\"skillsets/\" + skillset_name, body=json.dumps(skillset_json))\n",
    "    if result != None:\n",
    "        print(json.dumps(result, sort_keys=True, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will set up your main indexer.  This indexer will take the data from the json folder in your Azure blob container, run it through the skillset, and put the results in the search index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexer(indexer_name, filename):\n",
    "\n",
    "    # Define the request body\n",
    "    with open(filename) as datafile:\n",
    "      indexer_json = json.load(datafile)\n",
    "\n",
    "    try:\n",
    "        result = azsearch_rest(request_type=\"PUT\", endpoint=\"indexers/\" + indexer_name, body=json.dumps(indexer_json))\n",
    "        if result != None:\n",
    "            print(json.dumps(result, sort_keys=True, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:')\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "# Create main indexer\n",
    "indexer_name = search_prefix + \"-indexer\"\n",
    "create_indexer(indexer_name, filename=\"data-indexer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will set up your metadata indexer.  This indexer pulls the data from the metadata folder in your Azure blob container and adds it to the search index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_indexer_name = \"metadata-indexer\"\n",
    "create_indexer(metadata_indexer_name, filename=\"metadata-indexer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time running an indexer, you won't need to reset it.  But just in case you want to reuse this code and rerun your indexer with changes (perhaps pointing to your own dataset in Azure blob storage instead of ours), you will need to reset the indexer before making changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_indexer(indexer_name):\n",
    "    # Reset the indexer.\n",
    "    result = azsearch_rest(request_type=\"POST\", endpoint=\"/indexers/{0}/reset\".format(indexer_name), body=None)\n",
    "    if result != None:\n",
    "        print(json.dumps(result, sort_keys=True, indent=2))\n",
    "\n",
    "def run_indexer(indexer_name):\n",
    "    # Rerun the indexer.\n",
    "    result = azsearch_rest(request_type=\"POST\", endpoint=\"/indexers/{0}/run\".format(indexer_name), body=None)\n",
    "    if result != None:\n",
    "        print(json.dumps(result, sort_keys=True, indent=2))\n",
    "\n",
    "\n",
    "# Reset and rerun main indexer.  \n",
    "reset_indexer(indexer_name)\n",
    "run_indexer(indexer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and rerun the metadata indexer.\n",
    "reset_indexer(metadata_indexer_name)\n",
    "run_indexer(metadata_indexer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexer run can take a while, so let's check the status to see when it is ready.  Below we are checking the main indexer, not the metadata indexer, but you can do both if you want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "\n",
    "def check_indexer_status(indexer_name):\n",
    "    try:\n",
    "        complete = False\n",
    "        while (complete == False):\n",
    "            result = azsearch_rest(request_type=\"GET\", endpoint=\"indexers/{0}/status\".format(indexer_name))\n",
    "            state = result[\"status\"]\n",
    "            if result['lastResult'] is not None:\n",
    "                state = result['lastResult']['status']\n",
    "            print (state)\n",
    "            if state in (\"success\", \"error\"):\n",
    "                complete = True\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:')\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Check the main indexer\n",
    "check_indexer_status(indexer_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}